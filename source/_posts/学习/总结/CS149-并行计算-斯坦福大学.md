---
title: CS149-并行计算-斯坦福大学-学习笔记
date: 2025-01-17 00:07:21
updated: 2025-01-17 00:07:21
categories: 学习
tags:
  - 学习
  - 笔记
comments: true
toc: true
donate: true
share: true
---

# CS149-并行计算-斯坦福大学-学习笔记

- 课程录播（中英字幕，部分有误，fall 23）：https://www.bilibili.com/video/BV1du17YfE5G
- 课程官网（fall 24）：https://gfxcourses.stanford.edu/cs149/fall24
- 编程作业（fall24）：
  1. 在四核 CPU 上分析并行程序性能
  2. 在多核 CPU 上调度任务图
  3. 用 CUDA 实现一个简单的渲染器
  4. 在 DNN 加速硬件上优化深度神经网络（DNN）性能。（实现和优化 AWS Trainium 架构的内核）
  5. 使用 OpenMP 处理大规模图数据
  6. （fall23 原第三题）chat149: 小语言模型，用 C++ 实现基于 DNN 的 transformer 的注意力层（flash-attention）

# 其一：Why Parallelism? Why Efficiency?

> Fast != Efficient
>
> 速度快，不等于效率高

> 如果老板给了你 100 倍的算力设备，
> 你却仅仅将程序加快了 2 倍，
> 我想你就该被开除了。
>
> “是的，让他去重修 CS149。”

## demo1：第一个并行程序

> 案例：
>
> - 1 人可以在 45 秒内算出 16 个数的总和。（demo1：第一个并行程序）
> - 但 2 人并不能在 23 秒内算出 16 个数字的总和，而是 40 秒。
> - 4 人计算 16 个数字之和的时间，优化到了 19 秒，而不是 12 秒。（demo2：扩展到 4 个“处理器”）
> - 100 多人不能在 2 分钟内数清楚人数总和。（demo3：大批量并行执行）

这个案例告诉我们：
并行计算，关键不在于如何并行，如何计算（虽然这也很重要），关键在于数据传输和协同效率。

因此，我们更多的是在学习：如何传输数据，排列数据，而不是写代码或布局硬件。

## 什么是计算机程序？ What is a computer program ?

“程序（program）”不同于“代码（code）”。

程序，与食谱一样，应当是可执行的一系列命题。

计算机程序，则是“指令流”，或者叫“指令序列”。
必须是计算机认识的一系列 01 串。

## 处理器做了什么？ What does a processor do ?

一个简化的处理器有 3 个模块：

- Fetch / Decode： 取指令、解析指令
- ALU：算数运算单元
- Execution Context：执行上下文

## 案例：`a = x * x + y * y + z * z`

这个简单的多项式运算会编译出 5 条指令
（假设使用 r1，r2，r3 寄存器，
假设不使用任何编译优化
[编译优化（ChatGPT）](#编译优化chatgpt)
）：

1. mul r1, r1, r1
2. mul r2, r2, r2
3. mul r3, r3, r3
4. add r1, r1, r2 ; r1 = r1 + r2
5. add r1, r1, r3

我们发现，指令 4 依赖于指令 1、2；指令 5 依赖于指令 3、4。
所以，等待是必须的，即使使用 5 个 cpu，效率也不会超过 1 条 mul 指令时间加上 2 条 add 指令时间。

（这里还有寄存器最少使用原则，或最多使用原则，不做讨论）

如何进一步提高性能？

- 数学家：改进数学公式，减少乘法次数（耗时长的操作），增加加法次数（耗时短的操作），重新编写代码。
- 硬件工程师：指令重排，指令随机化，指令流水线，分支预测，多级缓存，自动并行化，自动向量化加速
- 软件工程师：编写高维度的，可并行化的程序。任务并行化，数据结构并行化，计算并行化，数据传输并行化，数据存储并行化。主动使用缓存（预存器）。

> 小结
>
> - 软件：物尽其用（压榨硬件的一切性能）
> - 硬件：用尽其物（挑选更好的硬件去适配软件）

## 编译优化（ChatGPT）

现代编译器通常会结合多种优化策略，来提高程序的执行效率。它们会根据不同的场景、硬件架构和程序的特征，选择不同的原则来优化寄存器的使用。以下是一些现代编译器普遍采用的优化原则和策略：

### 1. **寄存器最多使用原则（Maximizing Register Usage）**

现代编译器在许多情况下倾向于使用尽可能多的寄存器，这样可以减少对内存的访问，因为寄存器的访问速度远快于内存。这个原则通常用于以下情况：

- **频繁访问的变量**：如果某个变量在计算中被频繁使用，编译器会尽可能地将其保留在寄存器中，避免频繁的内存读写操作。
- **局部性原理**：寄存器的使用能很好地利用数据的局部性。例如，如果某个数据在一段时间内频繁访问，它可以存储在寄存器中，而不需要每次从内存中读取。

**举例**：对于一个简单的多项式计算：

```c
a = x * x + y * y + z * z;
```

编译器可能会为每个中间结果（如 `x * x`、`y * y`、`z * z`）分配一个寄存器，并将结果存储在寄存器中，避免额外的内存操作。

### 2. **寄存器分配和寄存器重命名**

- **寄存器分配**：编译器通过分析程序的数据流和生命周期来确定哪些变量应该保留在寄存器中。寄存器的分配不仅考虑变量的使用频率，还要避免寄存器冲突，尽可能地减少寄存器的交换和保存操作。
- **寄存器重命名**：现代编译器会使用寄存器重命名技术来避免因寄存器依赖（如数据依赖）而产生的性能瓶颈。通过给同一个物理寄存器分配多个逻辑寄存器，编译器可以让多个指令并行执行，避免等待依赖关系完成。

### 3. **局部变量优化**

现代编译器通常会通过 **局部性分析** 来优化寄存器的使用。局部变量是那些只在函数内部使用的变量，编译器会尽量将它们分配到寄存器中，而不是存储在栈上或堆上。特别是对于小型的、生命周期较短的局部变量，寄存器使用能显著提高性能。

**举例**：

```c
int sum = 0;
for (int i = 0; i < N; i++) {
    sum += arr[i];
}
```

编译器可能会将 `sum` 和 `i` 变量都存储在寄存器中，以减少对内存的访问。

### 4. **内联扩展（Inlining）**

编译器会尝试对一些小的、频繁调用的函数进行内联扩展（inline），将函数调用替换成函数的实现体。这可以消除函数调用的开销，并减少对寄存器和内存的使用，特别是在递归或频繁调用的情况下。

### 5. **循环优化（Loop Optimization）**

编译器对循环结构进行优化，以提高寄存器的利用率和减少内存访问。例如，编译器可能会将循环展开、循环重排或循环分割，以便使计算更密集，从而减少内存的读取次数。

**举例**：

```c
for (int i = 0; i < N; i++) {
    a[i] = b[i] + c[i];
}
```

编译器可能将其优化为多个操作并行执行，或者将内存访问合并，以更高效地利用寄存器。

### 6. **寄存器压力和溢出（Register Pressure and Spill）**

现代编译器会根据寄存器的使用情况动态调整优化策略。如果寄存器使用过多，导致“寄存器压力”过大，编译器就可能将某些值溢出（spill）到栈上，特别是在寄存器资源紧张的情况下。溢出操作会影响性能，因为它需要额外的内存读写操作。

### 7. **自动向量化**

现代编译器（如 GCC、Clang、Intel 的 ICC 编译器等）支持自动向量化。通过向量化，编译器可以将标量操作转化为 SIMD（单指令多数据流）操作，这样能够将多个计算任务并行地放到同一个寄存器组中，从而大幅提高计算效率。

**举例**：

```c
for (int i = 0; i < N; i++) {
    a[i] = b[i] + c[i];
}
```

如果编译器支持向量化，它可能会将这段代码转换成可以同时加法多个元素的 SIMD 指令，使得多个数组元素能够同时在寄存器中处理，从而提高性能。

### 8. **多级缓存和数据局部性**

现代编译器也会考虑缓存的优化，尽量减少从内存中读取的数据量，利用 CPU 的多级缓存（L1、L2、L3 缓存）。通过优化程序的数据访问模式和寄存器分配，编译器能够使数据尽可能保持在较快的缓存中，而不需要频繁访问较慢的主内存。

### 9. **并行计算和多核优化**

在多核处理器上运行时，编译器可能会自动将程序分割成多个并行线程，使用多核的并行性来加速计算。寄存器的分配和使用会考虑到多线程并发执行的情况，确保每个线程能够高效地利用本地寄存器。

### 10. **指令级并行性（ILP）优化**

编译器会使用一些高级技术（如指令重排、延迟槽填充、分支预测等）来优化指令级并行性。这些优化措施通过重新排列指令，减少指令间的依赖关系，从而更好地利用寄存器和其他硬件资源，提高程序的执行效率。

### 总结

现代编译器会综合考虑性能、内存带宽、寄存器数量、计算依赖关系等多种因素，使用寄存器最多使用原则来最大化寄存器的利用率，但也会根据具体的硬件架构和程序的特性做适当的调整。最终目标是减少内存访问、提高并行性，并通过智能的寄存器分配和优化技术提升整体性能。

# 其二：A Modern Muti-Core Processor

```C
void sinx(int N, int terms, float* x，float* y)
{
    for (int i=0; i<N; i++)
    {
        float value = x[i];
        float numer = x[i]*x[i]*x[i];
        int denom = 6; // 3!
        int sign = -1;
        for (int j=1; j<=terms; j++)
        {
            value += sign * numer / denom;
            numer *= x[i]*x[i];
            denom *=(2*j+2)*(2*j+3);
            sign *= -1;
        }
      y[i] = value;
    }
}
```

这段程序计算 `y[i] = sin(x[i])`，
N 为数组长度，
terms 则是精度（泰勒展开项个数）。

暂且不谈这段程序在优化方面，
利用了之前的计算结果来计算 x 的幂次与 j 的阶乘，
这种做法的精妙之处，
以及使用 for 循环和局部变量，而不是函数，
带来的更高程度的局部性，
也不考虑运算溢出问题。
我们只关心，如何并行化计算。
