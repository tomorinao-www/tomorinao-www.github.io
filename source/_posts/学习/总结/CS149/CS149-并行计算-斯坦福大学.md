---
title: CS149-并行计算-斯坦福大学-学习笔记
date: 2025-01-17 00:07:21
updated: 2025-01-17 00:07:21
categories: 学习
tags:
  - 学习
  - 笔记
comments: true
toc: true
donate: true
share: true
---

# CS149-并行计算-斯坦福大学-学习笔记

- 课程录播（中英字幕，部分有误，fall 23）：https://www.bilibili.com/video/BV1du17YfE5G
- 课程官网（fall 24）：https://gfxcourses.stanford.edu/cs149/fall24
- 编程作业（fall24）：
  1. 在四核 CPU 上分析并行程序性能
  2. 在多核 CPU 上调度任务图
  3. 用 CUDA 实现一个简单的渲染器
  4. 在 DNN 加速硬件上优化深度神经网络（DNN）性能。（实现和优化 AWS Trainium 架构的内核）
  5. 使用 OpenMP 处理大规模图数据
  6. （fall23 原第三题）chat149: 小语言模型，用 C++ 实现基于 DNN 的 transformer 的注意力层（flash-attention）

> **（fall24）课程导言**：
> 从智能手机到多核 CPU 和 GPU，再到世界上最大的超级计算机和网站，并行处理在现代计算中无处不在。本课程的目标是深入了解设计现代并行计算系统所涉及的基本原理和工程权衡，并教授有效利用这些机器所需的并行编程技术。因为编写好的并行程序需要了解关键的机器性能特征，所以本课程将涵盖并行硬件和软件设计。

# 其一：Why Parallelism? Why Efficiency?

> Fast != Efficient
>
> 速度快，不等于效率高

> 如果老板给了你 100 倍的算力设备，
> 你却仅仅将程序加快了 2 倍，
> 我想你就该被开除了。
>
> “是的，让他去重修 CS149。”

## demo1：第一个并行程序

> 案例：
>
> - 1 人可以在 45 秒内算出 16 个数的总和。（demo1：第一个并行程序）
> - 但 2 人并不能在 23 秒内算出 16 个数字的总和，而是 40 秒。
> - 4 人计算 16 个数字之和的时间，优化到了 19 秒，而不是 12 秒。（demo2：扩展到 4 个“处理器”）
> - 100 多人不能在 2 分钟内数清楚人数总和。（demo3：大批量并行执行）

这个案例告诉我们：
并行计算，关键不在于如何并行，如何计算（虽然这也很重要），关键在于数据传输和协同效率。

因此，我们更多的是在学习：如何传输数据，排列数据，而不是写代码或布局硬件。

## 什么是计算机程序？ What is a computer program ?

> A program is a list of processor instructions!

“程序（program）”不同于“代码（code）”。

程序，与食谱一样，应当是可执行的一系列命题（操作流程）。

计算机程序，则是“指令流”，或者叫“指令序列”。
必须是计算机认识的一系列 01 串。

## 处理器做了什么？ What does a processor do ?

一个简化的处理器有 3 个模块：

- Fetch / Decode： 取指令、解析指令
- ALU：算数运算单元
- Execution Context：执行上下文

## 案例：`a = x * x + y * y + z * z`

这个简单的多项式运算会编译出 5 条指令
（假设使用 r1，r2，r3 寄存器，
假设不使用任何编译优化
[编译优化（ChatGPT）](#编译优化chatgpt)
）：

1. mul r1, r1, r1
2. mul r2, r2, r2
3. mul r3, r3, r3
4. add r1, r1, r2 ; r1 = r1 + r2
5. add r1, r1, r3

我们发现，指令 4 依赖于指令 1、2；指令 5 依赖于指令 3、4。
所以，等待是必须的，即使使用 5 个 cpu，效率也不会超过 1 条 mul 指令时间加上 2 条 add 指令时间。

（这里还有寄存器最少使用原则，或最多使用原则，不做讨论）

如何进一步提高性能？

- 数学家：改进数学公式，减少乘法次数（耗时长的操作），增加加法次数（耗时短的操作），重新编写代码。
- 硬件工程师：指令重排，指令随机化，指令流水线，分支预测，多级缓存，自动并行化，自动向量化加速
- 软件工程师：编写高维度的，可并行化的程序。任务并行化，数据结构并行化，计算并行化，数据传输并行化，数据存储并行化。主动使用缓存（预存器）。

> 小结
>
> - 软件：物尽其用（压榨硬件的一切性能）
> - 硬件：用尽其物（挑选更好的硬件去适配软件）

## 编译优化（ChatGPT）

现代编译器通常会结合多种优化策略，来提高程序的执行效率。它们会根据不同的场景、硬件架构和程序的特征，选择不同的原则来优化寄存器的使用。以下是一些现代编译器普遍采用的优化原则和策略：

### 1. **寄存器最多使用原则（Maximizing Register Usage）**

现代编译器在许多情况下倾向于使用尽可能多的寄存器，这样可以减少对内存的访问，因为寄存器的访问速度远快于内存。这个原则通常用于以下情况：

- **频繁访问的变量**：如果某个变量在计算中被频繁使用，编译器会尽可能地将其保留在寄存器中，避免频繁的内存读写操作。
- **局部性原理**：寄存器的使用能很好地利用数据的局部性。例如，如果某个数据在一段时间内频繁访问，它可以存储在寄存器中，而不需要每次从内存中读取。

**举例**：对于一个简单的多项式计算：

```c
a = x * x + y * y + z * z;
```

编译器可能会为每个中间结果（如 `x * x`、`y * y`、`z * z`）分配一个寄存器，并将结果存储在寄存器中，避免额外的内存操作。

### 2. **寄存器分配和寄存器重命名**

- **寄存器分配**：编译器通过分析程序的数据流和生命周期来确定哪些变量应该保留在寄存器中。寄存器的分配不仅考虑变量的使用频率，还要避免寄存器冲突，尽可能地减少寄存器的交换和保存操作。
- **寄存器重命名**：现代编译器会使用寄存器重命名技术来避免因寄存器依赖（如数据依赖）而产生的性能瓶颈。通过给同一个物理寄存器分配多个逻辑寄存器，编译器可以让多个指令并行执行，避免等待依赖关系完成。

### 3. **局部变量优化**

现代编译器通常会通过 **局部性分析** 来优化寄存器的使用。局部变量是那些只在函数内部使用的变量，编译器会尽量将它们分配到寄存器中，而不是存储在栈上或堆上。特别是对于小型的、生命周期较短的局部变量，寄存器使用能显著提高性能。

**举例**：

```c
int sum = 0;
for (int i = 0; i < N; i++) {
    sum += arr[i];
}
```

编译器可能会将 `sum` 和 `i` 变量都存储在寄存器中，以减少对内存的访问。

### 4. **内联扩展（Inlining）**

编译器会尝试对一些小的、频繁调用的函数进行内联扩展（inline），将函数调用替换成函数的实现体。这可以消除函数调用的开销，并减少对寄存器和内存的使用，特别是在递归或频繁调用的情况下。

### 5. **循环优化（Loop Optimization）**

编译器对循环结构进行优化，以提高寄存器的利用率和减少内存访问。例如，编译器可能会将循环展开、循环重排或循环分割，以便使计算更密集，从而减少内存的读取次数。

**举例**：

```c
for (int i = 0; i < N; i++) {
    a[i] = b[i] + c[i];
}
```

编译器可能将其优化为多个操作并行执行，或者将内存访问合并，以更高效地利用寄存器。

### 6. **寄存器压力和溢出（Register Pressure and Spill）**

现代编译器会根据寄存器的使用情况动态调整优化策略。如果寄存器使用过多，导致“寄存器压力”过大，编译器就可能将某些值溢出（spill）到栈上，特别是在寄存器资源紧张的情况下。溢出操作会影响性能，因为它需要额外的内存读写操作。

### 7. **自动向量化**

现代编译器（如 GCC、Clang、Intel 的 ICC 编译器等）支持自动向量化。通过向量化，编译器可以将标量操作转化为 SIMD（单指令多数据流）操作，这样能够将多个计算任务并行地放到同一个寄存器组中，从而大幅提高计算效率。

**举例**：

```c
for (int i = 0; i < N; i++) {
    a[i] = b[i] + c[i];
}
```

如果编译器支持向量化，它可能会将这段代码转换成可以同时加法多个元素的 SIMD 指令，使得多个数组元素能够同时在寄存器中处理，从而提高性能。

### 8. **多级缓存和数据局部性**

现代编译器也会考虑缓存的优化，尽量减少从内存中读取的数据量，利用 CPU 的多级缓存（L1、L2、L3 缓存）。通过优化程序的数据访问模式和寄存器分配，编译器能够使数据尽可能保持在较快的缓存中，而不需要频繁访问较慢的主内存。

### 9. **并行计算和多核优化**

在多核处理器上运行时，编译器可能会自动将程序分割成多个并行线程，使用多核的并行性来加速计算。寄存器的分配和使用会考虑到多线程并发执行的情况，确保每个线程能够高效地利用本地寄存器。

### 10. **指令级并行性（ILP）优化**

编译器会使用一些高级技术（如指令重排、延迟槽填充、分支预测等）来优化指令级并行性。这些优化措施通过重新排列指令，减少指令间的依赖关系，从而更好地利用寄存器和其他硬件资源，提高程序的执行效率。

### 总结

现代编译器会综合考虑性能、内存带宽、寄存器数量、计算依赖关系等多种因素，使用寄存器最多使用原则来最大化寄存器的利用率，但也会根据具体的硬件架构和程序的特性做适当的调整。最终目标是减少内存访问、提高并行性，并通过智能的寄存器分配和优化技术提升整体性能。

# 其二：A Modern Muti-Core Processor

## 案例：计算 `y[i]=sin(x[i])`

```C
void sinx(int N, int terms, float* x, float* y)
{
    for (int i=0; i<N; i++)
    {
        float value = x[i];
        float numer = x[i] * x[i] * x[i];
        int denom = 6; // 3!
        int sign = -1;
        for (int j=1; j<=terms; j++)
        {
            value += sign * numer / denom;
            numer *= x[i] * x[i];
            denom *= (2*j + 2) * (2*j + 3);
            sign *= -1;
        }
      y[i] = value;
    }
}
```

这段程序计算 `y[i] = sin(x[i])`，
N 为数组长度，
terms 则是精度（泰勒展开项个数）。

暂且不谈这段程序在优化方面，
利用了之前的计算结果来计算 x 的幂次与 j 的阶乘，
这种做法的精妙之处，
以及使用 for 循环和局部变量，而不是函数，
带来的更高程度的局部性，
也不考虑运算溢出问题。
我们只关心，如何并行化计算。

#### 高维度并行化（调用方并行）

```C
typedef struct {
    int N;
    int terms;
    float* x;
    float* y;
} my_args;

void my_thread_func(my_args* args)
{
  sinx(args->N, args->terms, args->x, args->y); // do work
}

void parallel_sinx(int N, int terms, float* x, float* y)
{
  std::thread my_thread;
  my_args args;

  args.N = N/2;
  args.terms  = terms;
  args.x = x;
  args.y = y;

  my_thread = std::thread(my_thread_func, &args); // launch thread
  sinx(N - args.N, terms, x + args.N, y + args.N); // do work on main thread
  my_thread.join(); // wait for thread to complete
}
```

其中 `sinx()` 的定义同上。

老师给出的代码，
实现了 main 线程和子线程并行计算一串 `y[i]=sin(x[i])`。
主线程计算后半段，子线程计算前半段。
用指针运算，巧妙地在调用方进行了并行化
（即老师所说“更高维度的并行化”）。

但是，并不是所有程序都能做到这样良好的并行化。

#### 向量编程（底层并行）

AVX intrinsics
![向量编程 AVX intrinsics](/imgs/cs149/cs149-slide_034.jpg)

### 条件分支

![cs149-slide_039.jpg](/imgs/cs149/cs149-slide_039.jpg)

在现代向量运算器上，
比如 8 个单元的 ALU，
并不是每个单元都能有效利用
（每个单元都有自己的数据要处理，
有自己的条件判断分支）。
程序不可避免需要 if else 语句，
这就导致有的时候我们并不能充分发挥并行计算的全部性能
（也就是没做到“物尽其用”的硬件性能压榨）。

老师问：
有没有一种 if 语句，
能让并行化的性能反而降低为 1/8？

答案是：
if 里进行模 8 判断，
在 1/8 的情况下进入代码块 1，
在 7/8 的情况下进入代码块 2，
但代码块 1 所做的事情是代码块 2 的很多很多倍
（极端情况是代码块 1 执行所有计算，
代码块 2 不执行计算），
这样总体性能就约等于 1/8。

为了避免在大多数情况下进入低性能运算分支，
我们应当仔细考察运算过程，
写出“高并行化”的“高性能”代码。

## 现代多核处理器

### 3 种不同形式的并行执行

- **超标量（Superscalar）**：利用指令流中的 ILP。并行处理同一指令流中的不同指令（只用一个核心（core））。
  - 在执行过程中，硬件自动发现并行性。
- **单指令流多数据流（SIMD）（Single Instruction Multiple Data）**：一条指令控制多个 ALU（只用一个核心）。
  - 高效处理数据并行工作负载：摊销对多个 ALU 的控制成本。
  - 由编译器（显式 SIMD）或在运行时由硬件（隐式 SIMD）进行向量化
- **多核（Multi-core）**：使用多个处理核心
  - **提供线程级并行**：在每个核上同时执行完全不同的指令流
  - 软件创建线程以向硬件公开并行性（例如，通过线程 API）

### 现代多核处理器

回顾一下第一章的简单处理器架构：

- Fetch / Decode： 取指令、解析指令
- ALU（Execution）：算数运算单元（执行器）
- Execution Context：执行上下文

简单扩展一下，利用前面提到的三种并行化策略，
我们就可以得到：

- **单核、超标量处理器**：
  - 2 个取指器/解析器
  - 每个取指器对应 1 个执行器
  - 多个执行器共用一个上下文
- **多核处理器**：
  - 就是多个核心复制粘贴（不考虑数据、缓存交流等）
- **SIMD 四核处理器（八位宽）**：
  - 4 个核心
  - 每个核心 1 个取指器
  - 每个核心 8 个 ALU（8-wide SIMD）
  - 每个核心 1 个上下文
- > ![示例简单多核处理器](/imgs/cs149/cs149-slide_046.jpg)
- **四核 Intel i7-7700k CPU**:
  - 4 核心
  - 每个核心 1 个上下文
  - 每个核心有 3 个 8 位宽 SIMD ALUs（AVX2 instructions）（每个核心 3 个取指器，每个取指器 8 个 ALU）
  - 浮点运算性能：4（核） \* 8（位宽）\* 3（个）\* 4.2Ghz = 400GFLOPs
- > ![Intel i7-7700k CPU](/imgs/cs149/cs149-slide_047.jpg)
- **NVIDIA V100 GPU**
  - 80 个“SM（Streaming Multiprocessor）” 核心
  - 128 个 SIMD ALUs 每"SM"(@1.6 GHz)=16 TFLOPs (~250 Watts)

## 第二部分：访问内存（Part2：accessing memory）

#### 回想：非常长的数据访问延迟

延迟（在 4GHz 下的时钟周期数）

- L1 Cache：4
- L2 Cache：12
- L3 Cache：38
- DRAM：~248

#### 回想：缓存未命中

1. 冷启动未命中（Cold Miss / Compulsory Miss）
2. 容量未命中（Capacity Miss）
3. 冲突未命中（Conflict Miss / Collision Miss）
4. 一致性未命中（Coherence Miss / Invalid Miss）
5. 人为失效未命中（Manual Invalidation Miss）

#### 回想：数据预取

当访问大量连续性可预测的数据，
比如访问一大块数组时，
可以很容易进行预测，
并进行数据预取（data prefetch）

---

但是，如果近期没有访问过数据，
也很难预测程序下一步要干嘛，怎么办？

### 第 3 种方法：用多线程消除停顿

就像可以边烧水边炒菜一样，
当我们知道某种操作耗时，
我们就先去处理别的事情。

> 等水烧开，再去关火。

多线程，就是复制我们之前提到的处理器架构中的“上下文”。

- 线程 1：计算，访问内存，计算。
- 线程 2：计算，访问内存，计算。
- 线程 3：计算，访问内存，计算。
- 线程 4：计算，访问内存，计算。

使用一个核心，我们可以创建 4 个线程，
按顺序执行这些线程。
当线程 1 访问内存、等待数据的时候
（前面提到的大约 250 个时钟周期），
让线程 2 进行计算操作，
当线程 2 访问内存、等待数据的时候，
让线程 3 进行计算...

由此，我们得到了大致的多线程执行时间流：
![用多线程隐藏停顿](/imgs/cs149/cs149-slide_060.jpg)
这个程序的 cpu 利用率为：100%

> 等待，但不仅仅是等待

> 边等待，边计算

### 多线程，具体多少线程？

继续回想案例：计算一组 `y[i]=sin(x[i])`

假设：计算耗时 20%，传输数据耗时 80%。

假如只有一个线程，
那么 cpu 有 80% 的时间在 stall（停顿），
也就是等待内存读写（load/read memory）。

如果有 5 个线程，
那么在 1 个线程等待数据的 80%时间内，
剩下的 4 个线程可以完全占用 cpu，
当数据传输完毕，线程 1 回归计算操作，
cpu 利用率达到了 100%，
耗时缩短为原来的 20%，
从耗时结果上看，我们“消除了停顿”。

更多的线程不会带来更多的性能提升，
cpu 利用率已经到了 100%，
再增加线程反而会因为上下文切换，
导致耗时更多。

![多少线程？](/imgs/cs149/cs149-slide_069.jpg)

那么，当计算耗时占比为 `x`（x∈(0, 1)）时，应该设置几个线程？

先想象一个线程在执行，
数据传输时间占比为：`1-x`

为了占满等待时间，我们需要几个额外的线程呢？

那当然是 `(1-x)/x` 个啦。

所以总线程数是：`y = (1-x)/x + 1 = 1/x`

> **结论**：
> 当计算时间占比 `x`，线程数应设置为 `ceil(1/x)`。（ceil：向上取整）

### 硬件多线程

Hardware-supported multi-threading
硬件支持的多线程

- 硬件核心管理多个线程的执行上下文
  - 核心仍然拥有相同数量的 ALU 资源：多线程只有助于更有效地使用这些资源，在面对内存访问等高延迟操作时
  - 处理器决定每个时刻运行哪一个线程
- 交错多线程（也称时间多线程）
  Interleaved multi-threading (a.k.a. temporal multi-threading)
  - 每个时钟，核心选择一个线程，并运行这个线程的一条指令，在核心的 ALUs 上
- 同步多线程
  Simultaneous multi-threading (SMT)
  - 每个时钟，核心从多个线程中选择多条指令在 ALUs 上运行
  - 示例：英特尔超线程（Hyper-threading）（每个内核 2 个线程）

### 你应该知道的一些术语

- 指令流 Instruction stream
- 多核处理器 Multi-core processor
- SIMD 执行 SIMD execution
- 一致控制流 Coherent control flow
- 硬件多线程 Hardware multi-threading
  - 交错多线程 Interleaved multi-threading
  - 同时多线程 Simutaneous multi-threading

### 思考题：

你写了一个 C 语言应用程序，生成了 2 个线程

你的应用程序运行在如下处理器上：

- 2 个核心，每个核心 2 个执行上下文，每个时钟最多执行 （\_\_） 条指令，一条指令是 8 宽的 SIMD 指令。

- 问题 1：谁负责将应用程序的线程映射到处理器的线程执行上下文?
  - 答：操作系统（Operating system）
- 问题 2：如果你来实现一个操作系统，那么如何将这 2 个线程分配给 4 个执行上下文？
- 问题 3：如果您的 C 程序派生了 5 个线程，如何分配给执行上下文？
